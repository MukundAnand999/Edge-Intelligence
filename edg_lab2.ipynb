{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "14c5201e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: nltk in c:\\programdata\\anaconda3\\lib\\site-packages (3.7)\n",
      "Requirement already satisfied: joblib in c:\\users\\batch1\\appdata\\roaming\\python\\python39\\site-packages (from nltk) (1.4.2)\n",
      "Requirement already satisfied: click in c:\\programdata\\anaconda3\\lib\\site-packages (from nltk) (8.0.4)\n",
      "Requirement already satisfied: regex>=2021.8.3 in c:\\programdata\\anaconda3\\lib\\site-packages (from nltk) (2022.7.9)\n",
      "Requirement already satisfied: tqdm in c:\\programdata\\anaconda3\\lib\\site-packages (from nltk) (4.64.1)\n",
      "Requirement already satisfied: colorama in c:\\users\\batch1\\appdata\\roaming\\python\\python39\\site-packages (from click->nltk) (0.4.6)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4689f86",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.tokenize import word_tokenize, sent_tokenize\n",
    "nltk.download('punkt') # Required for the tokenizers\n",
    "\n",
    "text = \"NLTK is a powerful library. It helps with many NLP tasks.\"\n",
    "\n",
    "# Word Tokenization\n",
    "words = word_tokenize(text)\n",
    "print(f\"Words: {words}\")\n",
    "\n",
    "# Sentence Tokenization\n",
    "sentences = sent_tokenize(text)\n",
    "print(f\"Sentences: {sentences}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2aea53d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "nltk.download('stopwords') # Required for the stopwords list\n",
    "\n",
    "text = \"The quick brown fox jumps over the lazy dog.\"\n",
    "stop_words = set(stopwords.words(\"english\"))\n",
    "words = word_tokenize(text)\n",
    "\n",
    "filtered_words = [word for word in words if word.lower() not in stop_words]\n",
    "print(f\"Filtered words: {filtered_words}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d58d6986",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.stem import PorterStemmer\n",
    "from nltk.tokenize import word_tokenize\n",
    "nltk.download('punkt') # Required for tokenization\n",
    "\n",
    "stemmer = PorterStemmer()\n",
    "words = [\"running\", \"runs\", \"runner\", \"easily\", \"fairly\"]\n",
    "stemmed_words = [stemmer.stem(word) for word in words]\n",
    "print(f\"Stemmed words: {stemmed_words}\")\n",
    "# Output: ['run', 'run', 'runner', 'easili', 'fairli']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee980174",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "nltk.download('averaged_perceptron_tagger') # Required for POS tagging\n",
    "nltk.download('punkt') # Required for tokenization\n",
    "\n",
    "text = \"The woman bought a new car.\"\n",
    "words = word_tokenize(text)\n",
    "pos_tags = nltk.pos_tag(words)\n",
    "print(f\"POS tags: {pos_tags}\")\n",
    "# Output: [('The', 'DT'), ('woman', 'NN'), ('bought', 'VBD'), ('a', 'DT'), ('new', 'JJ'), ('car', 'NN'), ('.', '.')]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55383054",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "nltk.download('maxent_ne_chunker') # Required for NER\n",
    "nltk.download('words') # Required for NER\n",
    "nltk.download('averaged_perceptron_tagger') # Required for POS tagging\n",
    "\n",
    "text = \"Apple Inc. is a company located in Cupertino, California.\"\n",
    "words = word_tokenize(text)\n",
    "tagged_words = nltk.pos_tag(words)\n",
    "\n",
    "# Perform NER and view as a tree\n",
    "named_entities_tree = nltk.ne_chunk(tagged_words)\n",
    "print(\"Named Entities Tree:\")\n",
    "print(named_entities_tree)\n",
    "# The output will show a tree structure with 'ORGANIZATION' and 'LOCATION' identified.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "02467f2c",
   "metadata": {},
   "outputs": [
    {
     "ename": "LookupError",
     "evalue": "\n**********************************************************************\n  Resource \u001b[93mnames\u001b[0m not found.\n  Please use the NLTK Downloader to obtain the resource:\n\n  \u001b[31m>>> import nltk\n  >>> nltk.download('names')\n  \u001b[0m\n  For more information see: https://www.nltk.org/data.html\n\n  Attempted to load \u001b[93mcorpora/names\u001b[0m\n\n  Searched in:\n    - 'C:\\\\Users\\\\batch1/nltk_data'\n    - 'C:\\\\ProgramData\\\\Anaconda3\\\\nltk_data'\n    - 'C:\\\\ProgramData\\\\Anaconda3\\\\share\\\\nltk_data'\n    - 'C:\\\\ProgramData\\\\Anaconda3\\\\lib\\\\nltk_data'\n    - 'C:\\\\Users\\\\batch1\\\\AppData\\\\Roaming\\\\nltk_data'\n    - 'C:\\\\nltk_data'\n    - 'D:\\\\nltk_data'\n    - 'E:\\\\nltk_data'\n**********************************************************************\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mLookupError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\nltk\\corpus\\util.py\u001b[0m in \u001b[0;36m__load\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m     83\u001b[0m                 \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 84\u001b[1;33m                     \u001b[0mroot\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnltk\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfind\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34mf\"{self.subdir}/{zip_name}\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     85\u001b[0m                 \u001b[1;32mexcept\u001b[0m \u001b[0mLookupError\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\nltk\\data.py\u001b[0m in \u001b[0;36mfind\u001b[1;34m(resource_name, paths)\u001b[0m\n\u001b[0;32m    582\u001b[0m     \u001b[0mresource_not_found\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34mf\"\\n{sep}\\n{msg}\\n{sep}\\n\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 583\u001b[1;33m     \u001b[1;32mraise\u001b[0m \u001b[0mLookupError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mresource_not_found\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    584\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mLookupError\u001b[0m: \n**********************************************************************\n  Resource \u001b[93mnames\u001b[0m not found.\n  Please use the NLTK Downloader to obtain the resource:\n\n  \u001b[31m>>> import nltk\n  >>> nltk.download('names')\n  \u001b[0m\n  For more information see: https://www.nltk.org/data.html\n\n  Attempted to load \u001b[93mcorpora/names.zip/names/\u001b[0m\n\n  Searched in:\n    - 'C:\\\\Users\\\\batch1/nltk_data'\n    - 'C:\\\\ProgramData\\\\Anaconda3\\\\nltk_data'\n    - 'C:\\\\ProgramData\\\\Anaconda3\\\\share\\\\nltk_data'\n    - 'C:\\\\ProgramData\\\\Anaconda3\\\\lib\\\\nltk_data'\n    - 'C:\\\\Users\\\\batch1\\\\AppData\\\\Roaming\\\\nltk_data'\n    - 'C:\\\\nltk_data'\n    - 'D:\\\\nltk_data'\n    - 'E:\\\\nltk_data'\n**********************************************************************\n",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[1;31mLookupError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_12620\\3020970201.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mnltk\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcorpus\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mnames\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 6\u001b[1;33m \u001b[0mnames\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mwods\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      7\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnames\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mwords\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\nltk\\corpus\\util.py\u001b[0m in \u001b[0;36m__getattr__\u001b[1;34m(self, attr)\u001b[0m\n\u001b[0;32m    119\u001b[0m             \u001b[1;32mraise\u001b[0m \u001b[0mAttributeError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"LazyCorpusLoader object has no attribute '__bases__'\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    120\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 121\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__load\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    122\u001b[0m         \u001b[1;31m# This looks circular, but its not, since __load() changes our\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    123\u001b[0m         \u001b[1;31m# __class__ to something new:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\nltk\\corpus\\util.py\u001b[0m in \u001b[0;36m__load\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m     84\u001b[0m                     \u001b[0mroot\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnltk\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfind\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34mf\"{self.subdir}/{zip_name}\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     85\u001b[0m                 \u001b[1;32mexcept\u001b[0m \u001b[0mLookupError\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 86\u001b[1;33m                     \u001b[1;32mraise\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     87\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     88\u001b[0m         \u001b[1;31m# Load the corpus.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\nltk\\corpus\\util.py\u001b[0m in \u001b[0;36m__load\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m     79\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     80\u001b[0m             \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 81\u001b[1;33m                 \u001b[0mroot\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnltk\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfind\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34mf\"{self.subdir}/{self.__name}\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     82\u001b[0m             \u001b[1;32mexcept\u001b[0m \u001b[0mLookupError\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     83\u001b[0m                 \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\nltk\\data.py\u001b[0m in \u001b[0;36mfind\u001b[1;34m(resource_name, paths)\u001b[0m\n\u001b[0;32m    581\u001b[0m     \u001b[0msep\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m\"*\"\u001b[0m \u001b[1;33m*\u001b[0m \u001b[1;36m70\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    582\u001b[0m     \u001b[0mresource_not_found\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34mf\"\\n{sep}\\n{msg}\\n{sep}\\n\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 583\u001b[1;33m     \u001b[1;32mraise\u001b[0m \u001b[0mLookupError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mresource_not_found\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    584\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    585\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mLookupError\u001b[0m: \n**********************************************************************\n  Resource \u001b[93mnames\u001b[0m not found.\n  Please use the NLTK Downloader to obtain the resource:\n\n  \u001b[31m>>> import nltk\n  >>> nltk.download('names')\n  \u001b[0m\n  For more information see: https://www.nltk.org/data.html\n\n  Attempted to load \u001b[93mcorpora/names\u001b[0m\n\n  Searched in:\n    - 'C:\\\\Users\\\\batch1/nltk_data'\n    - 'C:\\\\ProgramData\\\\Anaconda3\\\\nltk_data'\n    - 'C:\\\\ProgramData\\\\Anaconda3\\\\share\\\\nltk_data'\n    - 'C:\\\\ProgramData\\\\Anaconda3\\\\lib\\\\nltk_data'\n    - 'C:\\\\Users\\\\batch1\\\\AppData\\\\Roaming\\\\nltk_data'\n    - 'C:\\\\nltk_data'\n    - 'D:\\\\nltk_data'\n    - 'E:\\\\nltk_data'\n**********************************************************************\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "# def gender feature(word)\n",
    "    \n",
    "\n",
    "from nltk.corpus import names\n",
    "names.wods()\n",
    "print(len(names.words()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "eba5c062",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[239 409   3 805 421 367 925  24 757  82 756 795 823 569  17  82  46 382\n",
      "  62 157 292 319 477  59 333 581 939 776  18 286 877 865 847 723 991 895\n",
      " 152 580 384 811  93 459 382 750 214 321 992 256 297 770 466 633  69 191\n",
      " 551 483 763 419 625 420 189 225 269 411 359 217 147  78 992 849 339 213\n",
      " 695 172 140 752 746 659 677 868 668 695 812 291 726 768  19 612 467 566\n",
      " 393 929 757 651 723 731 265 640 465 487]\n",
      "      0\n",
      "0   239\n",
      "1   409\n",
      "2     3\n",
      "3   805\n",
      "4   421\n",
      "..  ...\n",
      "95  731\n",
      "96  265\n",
      "97  640\n",
      "98  465\n",
      "99  487\n",
      "\n",
      "[100 rows x 1 columns]\n",
      "      0\n",
      "0   239\n",
      "1   409\n",
      "2     3\n",
      "3   805\n",
      "4   421\n",
      "5   367\n",
      "6   925\n",
      "7    24\n",
      "8   757\n",
      "9    82\n",
      "10  756\n",
      "11  795\n",
      "12  823\n",
      "13  569\n",
      "14   17\n",
      "15   82\n",
      "16   46\n",
      "17  382\n",
      "18   62\n",
      "19  157\n",
      "20  292\n",
      "21  319\n",
      "22  477\n",
      "23   59\n",
      "24  333\n",
      "25  581\n",
      "26  939\n",
      "27  776\n",
      "28   18\n",
      "29  286\n",
      "30  877\n",
      "31  865\n",
      "32  847\n",
      "33  723\n",
      "34  991\n",
      "35  895\n",
      "36  152\n",
      "37  580\n",
      "38  384\n",
      "39  811\n",
      "40   93\n",
      "41  459\n",
      "42  382\n",
      "43  750\n",
      "44  214\n",
      "45  321\n",
      "46  992\n",
      "47  256\n",
      "48  297\n",
      "49  770\n",
      "      0\n",
      "0   239\n",
      "1   409\n",
      "2     3\n",
      "3   805\n",
      "4   421\n",
      "5   367\n",
      "6   925\n",
      "7    24\n",
      "8   757\n",
      "9    82\n",
      "10  756\n",
      "11  795\n",
      "12  823\n",
      "13  569\n",
      "14   17\n",
      "15   82\n",
      "16   46\n",
      "17  382\n",
      "18   62\n",
      "19  157\n",
      "20  292\n",
      "21  319\n",
      "22  477\n",
      "23   59\n",
      "24  333\n",
      "25  581\n",
      "26  939\n",
      "27  776\n",
      "28   18\n",
      "29  286\n",
      "30  877\n",
      "31  865\n",
      "32  847\n",
      "33  723\n",
      "34  991\n",
      "35  895\n",
      "36  152\n",
      "37  580\n",
      "38  384\n",
      "39  811\n",
      "40   93\n",
      "41  459\n",
      "42  382\n",
      "43  750\n",
      "44  214\n",
      "45  321\n",
      "46  992\n",
      "47  256\n",
      "48  297\n",
      "49  770\n",
      "      0\n",
      "0   239\n",
      "1   409\n",
      "2     3\n",
      "3   805\n",
      "4   421\n",
      "5   367\n",
      "6   925\n",
      "7    24\n",
      "8   757\n",
      "9    82\n",
      "10  756\n",
      "11  795\n",
      "12  823\n",
      "13  569\n",
      "14   17\n",
      "15   82\n",
      "16   46\n",
      "17  382\n",
      "18   62\n",
      "19  157\n"
     ]
    }
   ],
   "source": [
    "import random \n",
    "# random.shuffle(labelled_names)\n",
    "# featuresets = [(geder_featruesets)]\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "# arr = np.random.randint(0, 100, size=10)\n",
    "arr = np.random.randint(0,1000, size = 100)\n",
    "print(arr)\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "s = pd.DataFrame(arr)\n",
    "print(s)\n",
    "\n",
    "print(s[:50])\n",
    "\n",
    "train_set,test_set = s[:50], s[:20]\n",
    "\n",
    "print(train_set)\n",
    "print(test_set)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1fc02207",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "def gender_feature(word):\n",
    "    return{'last'}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e05edcc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "# def gender feature(word)\n",
    "    \n",
    "\n",
    "from nltk.corpus import names\n",
    "names.wods()\n",
    "print(len(names.words()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a945ea48",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "random.sufle(labelled_names)\n",
    "feturesets =[(gender_featureset(n),gender) for (n, gender) in labelled_names]\n",
    "train_set,test_set = featuresets[:5000],featuresets[:200]\n",
    "import nltk\n",
    "\n",
    "classifier = nltk.NaiveBayesClassifier.train(train_set)\n",
    "classifier.classify(gender_feature('David')\n",
    "classifier.classify(gender_feature('Yann Lecum')\n",
    "classifier.classify(gender_feature('Yann Lecum')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "a6328359",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                    @timestamp                   _id  ip_address\n",
      "0  July 8th 2019, 14:43:03.000  XswJ0msBoTGddM7vxMDB  10.1.1.285\n",
      "1  July 8th 2019, 14:43:01.000  dKQJ0msB7mP0GwVzvJjz  10.1.2.389\n",
      "2  July 8th 2019, 14:42:59.000  CcwJ0msBoTGddM7vtb8y  10.1.1.415\n",
      "3  July 8th 2019, 14:42:57.000  bKQJ0msB7mP0GwVzrZdT   10.1.1.79\n",
      "4  July 8th 2019, 14:42:55.000  L6QJ0msB7mP0GwVzpZeI   10.1.1.60\n",
      "(721547, 3)\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd \n",
    "df = pd.read_csv(\"C:\\\\Users\\\\batch1\\\\Desktop\\\\muku\\\\logs_dataset.csv\")\n",
    "\n",
    "print(df.head())\n",
    "\n",
    "print(df.shape)\n",
    "print\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a21f2a8",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
